@article{Niculae2017,
	abstract = {Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a max operator regularized with a strongly convex function. We show that this operator is differentiable and that its gradient defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in new attention mechanisms that focus on entire segments or groups of an input, encouraging parsimony and interpretability. We derive efficient algorithms to compute the forward and backward passes of these attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing attention mechanisms, we evaluate them on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the existing attention mechanisms based on softmax and sparsemax.},
	archivePrefix = {arXiv},
	arxivId = {1705.07704},
	author = {Niculae, Vlad and Blondel, Mathieu},
	eprint = {1705.07704},
	file = {:home/skarita/Downloads/1705.07704.pdf:pdf},
	journal = {ArXiv e-prints},
	mendeley-groups = {NTT},
	title = {{A Regularized Framework for Sparse and Structured Neural Attention}},
	url = {http://arxiv.org/abs/1705.07704},
	year = {2017}
}
@article{Cuturi2017,
	abstract = {We propose in this paper a differentiable learning loss between time series. Our proposal builds upon the celebrated Dynamic Time Warping (DTW) discrepancy. Unlike the Euclidean distance, DTW is able to compare asynchronous time series of varying size and is robust to elastic transformations in time. To be robust to such invariances, DTW computes a minimal cost alignment between time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and its gradient can be computed with quadratic time/space complexity (DTW has quadratic time and linear space complexity). We show that our regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.},
	archivePrefix = {arXiv},
	arxivId = {1703.01541},
	author = {Cuturi, Marco and Blondel, Mathieu},
	eprint = {1703.01541},
	file = {:home/skarita/Downloads/1703.01541.pdf:pdf},
	mendeley-groups = {NTT},
	month = {mar},
	number = {1998},
	pages = {1--27},
	title = {{Soft-DTW: a Differentiable Loss Function for Time-Series}},
	url = {http://arxiv.org/abs/1703.01541},
	year = {2017}
}
@inproceedings{Povey2002,
	author = {Povey, D and Woodland, P.C.},
	booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing},
	doi = {10.1109/ICASSP.2002.5743665},
	file = {:home/skarita/Downloads/05743665.pdf:pdf},
	isbn = {0-7803-7402-9},
	mendeley-groups = {NTT/ASRU17},
	month = {may},
	number = {6},
	pages = {I--105--I--108},
	publisher = {IEEE},
	title = {{Minimum Phone Error and I-smoothing for improved discriminative training}},
	url = {http://ieeexplore.ieee.org/document/5743665/},
	year = {2002}
}
@book{Bourlard1994,
	abstract = {Connectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state of the art continuous speech recognition systems based on hidden Markov models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e. HMM emission probability estimation and feature extraction. The book describes a successful five-year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical systems. Using standard databases and comparison with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. The book is also suitable as a text for advanced courses on neural networks or speech processing.},
	address = {Boston, MA},
	author = {Bourlard, Herv{\'{e}} A. and Morgan, Nelson},
	booktitle = {International Journal on Pattern Recognition and Artificial Intelligence},
	doi = {10.1007/978-1-4615-3210-1},
	file = {:home/skarita/Downloads/10.1.1.646.9055.pdf:pdf},
	isbn = {978-1-4613-6409-2},
	mendeley-groups = {NTT/ASRU17},
	number = {4},
	pages = {647--668},
	publisher = {Springer US},
	title = {{Connectionist speech recognition}},
	url = {http://books.google.com/books?hl=en{\&}lr={\&}id=7vm{\_}8bPHCEUC{\&}oi=fnd{\&}pg=PA3{\&}dq=CONNECTIONIST+SPEECH+RECOGNITION+A+Hybrid+Approach{\&}ots=51As5XQCZf{\&}sig=cQUbax8BqXQGqjjs68ybtW7Llfs http://link.springer.com/10.1007/978-1-4615-3210-1},
	volume = {247},
	year = {1994}
}
@inproceedings{Graves2013,
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	annote = {NULL},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1303.5778v1},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
	doi = {10.1109/ICASSP.2013.6638947},
	eprint = {arXiv:1303.5778v1},
	file = {:home/skarita/Documents/Mendeley Desktop/2013 - Speech recognition with deep recurrent neural networks - 2013 IEEE International Conference on Acoustics, Speech and Signal Proce.pdf:pdf},
	isbn = {978-1-4799-0356-6},
	issn = {1520-6149},
	mendeley-groups = {NTT/TIMIT sa,NTT},
	month = {may},
	number = {3},
	pages = {6645--6649},
	publisher = {IEEE},
	title = {{Speech recognition with deep recurrent neural networks}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6638947},
	year = {2013}
}
@article{Sutskever2014,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	archivePrefix = {arXiv},
	arxivId = {1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	doi = {10.1007/s10107-014-0839-0},
	eprint = {1409.3215},
	file = {:home/skarita/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
	isbn = {1409.3215},
	issn = {09205691},
	journal = {Advances in Neural Information Processing Systems (NIPS)},
	mendeley-groups = {NTT/end-to-end/q},
	pages = {3104--3112},
	pmid = {2079951},
	title = {{Sequence to sequence learning with neural networks}},
	url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
	year = {2014}
}
@article{Kingma2014,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	archivePrefix = {arXiv},
	arxivId = {1412.6980},
	author = {Kingma, Diederik and Ba, Jimmy},
	eprint = {1412.6980},
	file = {:home/skarita/Documents/Mendeley Desktop/2014 - Adam A Method for Stochastic Optimization - International Conference on Learning Representations - Kingma, Ba.pdf:pdf},
	journal = {International Conference on Learning Representations},
	mendeley-groups = {NTT/DNN,NTT/Optimization},
	pages = {1--13},
	title = {{Adam: a method for stochastic optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	year = {2014}
}
@article{Xu2009,
	author = {Xu, Haihua and Povey, Daniel and Zhu, Jie and Wu, Guanyong},
	file = {:home/skarita/Downloads/interspeech09.pdf:pdf},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Confusion networks,Lattice rescoring,MPE,Minimum bayes risk (MBR),Speech recognition},
	mendeley-groups = {NTT/ASRU17},
	number = {4},
	pages = {76--79},
	title = {{Minimum hypothesis phone error as a decoding method for speech recognition}},
	year = {2009}
}
@InProceedings{ROUSSEAU12.698,
    author    = {Anthony Rousseau and
               Paul Del{\'{e}}glise and
               Yannick Est{\`{e}}ve},
	title = {TED-LIUM: an automatic speech recognition dedicated corpus},
	booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
	year = {2012},
	month = {may},
	date = {23-25},
	address = {Istanbul, Turkey},
	publisher = {European Language Resources Association (ELRA)},
	isbn = {978-2-9517408-7-7},
	language = {english}
}
@INPROCEEDINGS{Povey_ASRU2011_2011,
	author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
	keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
	projects = {Idiap, IM2},
	month = dec,
	title = {The kaldi speech recognition toolkit},
	booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
	year = {2011},
	publisher = {IEEE Signal Processing Society},
	location = {Hilton Waikoloa Village, Big Island, Hawaii, US},
	note = {IEEE Catalog No.: CFP11SRW-USB},
	isbn = {978-1-4673-0366-8},
	abstract = {We describe the design of Kaldi, a free, open-source
	toolkit for speech recognition research. Kaldi provides a speech
	recognition system based on finite-state transducers (using the
	freely available OpenFst), together with detailed documentation
	and scripts for building complete recognition systems. Kaldi
	is written is C++, and the core library supports modeling of
	arbitrary phonetic-context sizes, acoustic modeling with subspace
	Gaussian mixture models (SGMM) as well as standard Gaussian
	mixture models, together with all commonly used linear and
	affine transforms. Kaldi is released under the Apache License
	v2.0, which is highly nonrestrictive, making it suitable for a wide
	community of users.},
	pdf = {http://publications.idiap.ch/downloads/papers/2012/Povey_ASRU2011_2011.pdf}
}
@inproceedings{Luo2017,
	author = {Luo, Yuping and Chiu, Chung-cheng and Jaitly, Navdeep and Sutskever, Ilya},
	booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	doi = {10.1109/ICASSP.2017.7952667},
	file = {:home/skarita/Downloads/07952667.pdf:pdf},
	isbn = {978-1-5090-4117-6},
	mendeley-groups = {NTT/ASRU17},
	month = {mar},
	pages = {2801--2805},
	publisher = {IEEE},
	title = {{Learning online alignments with continuous rewards policy gradient}},
	url = {http://ieeexplore.ieee.org/document/7952667/},
	year = {2017}
}
@article{Silver2016,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	archivePrefix = {arXiv},
	arxivId = {1610.00633},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	doi = {10.1038/nature16961},
	eprint = {1610.00633},
	file = {:home/skarita/Downloads/AlphaGoNaturePaper.pdf:pdf},
	isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
	issn = {0028-0836},
	journal = {Nature},
	mendeley-groups = {NTT/ASRU17},
	month = {jan},
	number = {7587},
	pages = {484--489},
	pmid = {26819042},
	publisher = {Nature Publishing Group},
	title = {{Mastering the game of Go with deep neural networks and tree search}},
	url = {http://www.nature.com/doifinder/10.1038/nature16961},
	volume = {529},
	year = {2016}
}
@inproceedings{xuc15,
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	address = {Lille, France},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	editor = {Bach, Francis and Blei, David},
	file = {:home/skarita/Documents/Mendeley Desktop/2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention - Icml-2015 - Xu et al.pdf:pdf},
	mendeley-groups = {NTT/ASRU17},
	pages = {2048--2057},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{Show, attend and tell: neural image caption generation with visual attention}},
	url = {http://proceedings.mlr.press/v37/xuc15.html},
	volume = {37},
	year = {2015}
}
@INPROCEEDINGS{Sak2015,
	author={H. Sak and A. Senior and K. Rao and O. \.Irsoy and A. Graves and F. Beaufays and J. Schalkwyk},
	booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	title={Learning acoustic frame labeling for speech recognition with recurrent neural networks},
	year={2015},
	pages={4280-4284},
	keywords={entropy;hidden Markov models;learning (artificial intelligence);recurrent neural nets;speech recognition;CE training;CTC method;HMM state;LSTM RNN model;connectionist temporal classification method;cross-entropy training;finite state transducer;learning acoustic frame labeling;long short-term memory recurrent neural network;sMBR training;sequence discriminative training;speech recognition;Acoustics;Context modeling;Gold;Hidden Markov models;Neural networks;Speech recognition;Training;CTC;LSTM;RNN;acoustic modeling},
	doi={10.1109/ICASSP.2015.7178778},
	ISSN={1520-6149},
	month={April},}
@article{Gibson2006,
	abstract = {The Minimum Bayes Risk (MBR) framework has been a successful strategy for the training of hidden Markov models for large vocabulary speech recognition. Practical implementations of MBR must select an appropriate hypothesis space and loss function. The set of word sequences and a word-based Levenshtein distance may be assumed to be the optimal choice but use of phoneme-based criteria appears to be more successful. This paper compares the use of different hypothesis spaces and loss functions defined using the system constituents of word, phone, physical triphone, physical state and physical mixture component. For practical reasons the competing hypotheses are constrained by sampling. The impact of the sampling technique on the performance of MBR training is also examined.},
	author = {Gibson, Matthew and Hain, Thomas},
	file = {:home/skarita/Downloads/i06{\_}1653.pdf:pdf},
	isbn = {9781604234497},
	journal = {INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
	keywords = {Acoustic signal processing,Computer networks,Fault detection,Hidden Markov models,Linguistics,Markov processes,Risk perception,Set theory,Speech,Speech analysis,Telephone systems,Vocabulary control},
	mendeley-groups = {NTT/SequentialTraining},
	number = {3},
	pages = {2406--2409},
	title = {{Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition}},
	volume = {5},
	year = {2006}
}
@article{Vesely2013,
	abstract = {Sequence-discriminative training of deep neural networks (DNNs) is investigated on a 300 hour American English conversational telephone speech task. Different sequence-discriminative criteria - maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI - are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria - lattices are re-generated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hypotheses are disjoint are removed from the gradient computation. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 8-9{\%} relative, on average. Little difference is noticed between the different sequence-based criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results.},
	annote = {NULL},
	author = {Vesel{\'{y}}, Karel and Ghoshal, Arnab and Burget, Luk{\'{a}}{\v{s}} and Povey, Daniel},
	doi = {10.1109/ICASSP.2013.6639310},
	file = {:home/skarita/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vesel{\'{y}} et al. - 2013 - Sequence-discriminative training of deep neural networks.pdf:pdf},
	isbn = {9781479927562},
	issn = {19909772},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	keywords = {Deep learning,Neural networks,Reproducible research,Sequence-criterion training,Speech recognition},
	mendeley-groups = {NTT/SequentialTraining,NTT/SequentialTraining/ASR,NTT/ASRU17},
	number = {1},
	pages = {2345--2349},
	title = {{Sequence-discriminative training of deep neural networks}},
	year = {2013}
}
@inproceedings{Povey2008,
	abstract = {We present a modified form of the Maximum Mutual Information (MMI) objective function which gives improved results for discrim-inative training. The modification consists of boosting the likeli-hoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript, by using the same phone accu-racy function that is used in Minimum Phone Error (MPE) training. We combine this with another improvement to our implementation of the Extended Baum-Welch update equations for MMI, namely the canceling of any shared part of the numerator and denominator statistics on each frame (a procedure that is already done in MPE). This change affects the Gaussian-specific learning rate. We also in-vestigate another modification whereby we replace I-smoothing to the ML estimate with I-smoothing to the previous iteration's value. Boosted MMI gives better results than MPE in both model and feature-space discriminative training, although not consistently.},
	author = {Povey, Daniel and Kanevsky, Dimitri and Kingsbury, Brian and Ramabhadran, Bhuvana and Saon, George and Visweswariah, Karthik},
	booktitle = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
	doi = {10.1109/ICASSP.2008.4518545},
	file = {:home/skarita/Downloads/icassp08{\_}mmi.pdf:pdf},
	isbn = {978-1-4244-1483-3},
	issn = {1520-6149},
	mendeley-groups = {NTT/ASRU17},
	month = {mar},
	number = {ii},
	pages = {4057--4060},
	publisher = {IEEE},
	title = {{Boosted MMI for model and feature-space discriminative training}},
	url = {http://ieeexplore.ieee.org/document/4518545/},
	year = {2008}
}
@article{Saon2017,
	abstract = {One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14{\%} have dropped to 8.0{\%}, then 6.6{\%} and most recently 5.8{\%}, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5{\%}/10.3{\%} on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.},
	archivePrefix = {arXiv},
	arxivId = {1703.02136},
	author = {Saon, George and Kurata, Gakuto and Sercu, Tom and Audhkhasi, Kartik and Thomas, Samuel and Dimitriadis, Dimitrios and Cui, Xiaodong and Ramabhadran, Bhuvana and Picheny, Michael and Lim, Lynn-Li and Roomi, Bergul and Hall, Phil},
	eprint = {1703.02136},
	file = {:home/skarita/Downloads/1703.02136.pdf:pdf},
	journal = {arXiv Preprint},
	mendeley-groups = {NTT/ASRU17},
	title = {{English conversational telephone speech recognition by humans and machines}},
	url = {http://arxiv.org/abs/1703.02136},
	year = {2017}
}
@article{Bahdanau2017,
	abstract = {We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a $\backslash$textit{\{}critic{\}} network that is trained to predict the value of an output token, given the policy of an $\backslash$textit{\{}actor{\}} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.},
	archivePrefix = {arXiv},
	arxivId = {1607.07086},
	author = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
	eprint = {1607.07086},
	file = {:home/skarita/Downloads/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf:pdf},
	journal = {International Conference on Learning Representation (ICLR)},
	keywords = {finite element analysis,indentation,plasticity},
	mendeley-groups = {NTT/ASRU17},
	month = {jul},
	title = {{An actor-critic algorithm for sequence prediction}},
	url = {http://arxiv.org/abs/1412.7755 http://arxiv.org/abs/1607.07086},
	year = {2017}
}
@article{Ranzato2016,
	abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
	annote = {NULL},
	archivePrefix = {arXiv},
	arxivId = {1511.06732},
	author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	eprint = {1511.06732},
	file = {:home/skarita/Downloads/1511.06732.pdf:pdf},
	journal = {International Conference on Learning Representation (ICLR)},
	keywords = {Optimization,RNN},
	mendeley-groups = {NTT/SequentialTraining/MT,NTT/ASRU17},
	title = {{Sequence level training with recurrent neural networks}},
	url = {http://arxiv.org/abs/1511.06732 https://github.com/facebookresearch/MIXER},
	year = {2016}
}
@article{Bahdanau2016,
    abstract = {Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural net-works and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which neural architectures were trained to model sequences of char-acters [1, 2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We in-vestigate an alternative method for sequence modelling based on an attention mechanism that allows a Recurrent Neural Network (RNN) to learn alignments between sequences of input frames and output labels. We show how this setup can be applied to LVCSR by integrating the decoding RNN with an n-gram language model and by speeding up its operation by constraining selections made by the attention mechanism and by reducing the source sequence lengths by pooling in-formation over time. Recognition accuracies similar to other HMM-free RNN-based approaches are reported for the Wall Street Journal corpus.},
    author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Bengio, Yoshua},
    file = {:C$\backslash$:/Users/skarita/Downloads/0004945.pdf:pdf},
    isbn = {9781479999880},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
    mendeley-groups = {NTT/ASRU17},
    pages = {4945--4949},
    title = {{End-to-end attention-based large vocabulary speech recognition}},
    year = {2016}
}
@inproceedings{Kim2017,
    abstract = {Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6{\%} relative improvements in Character Error Rate (CER).},
    archivePrefix = {arXiv},
    arxivId = {1609.06773},
    author = {Kim, Suyoun and Hori, Takaaki and Watanabe, Shinji},
    booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
    eprint = {1609.06773},
    file = {:C$\backslash$:/Users/skarita/Downloads/0004835.pdf:pdf},
    isbn = {9781509041176},
    mendeley-groups = {NTT/ASRU17},
    pages = {4835--4839},
    title = {{Joint CTC-attention based end-to-end speech recognition using multi-task learning}},
    url = {http://arxiv.org/abs/1609.06773},
    year = {2017}
}
@article{Chan2016,
    abstract = {We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1{\%} without a dictionary or an external language model and 10.3{\%} with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%} on the same set.},
    author = {Chan, William and Jaitly, Navdeep and Le, Quoc and Vinyals, Oriol},
    doi = {10.1109/ICASSP.2016.7472621},
    file = {:C$\backslash$:/Users/skarita/Downloads/0004960.pdf:pdf},
    isbn = {9781479999880},
    issn = {15206149},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    keywords = {Recurrent neural network,end-to-end speech recognition,neural attention},
    mendeley-groups = {NTT/ASRU17},
    pages = {4960--4964},
    title = {{Listen, attend and spell: A neural network for large vocabulary conversational speech recognition}},
    volume = {2016-May},
    year = {2016}
}
@article{Zhang2017,
    abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
    archivePrefix = {arXiv},
    arxivId = {1610.03022},
    author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
    eprint = {1610.03022},
    file = {:C$\backslash$:/Users/skarita/Downloads/0004845.pdf:pdf},
    isbn = {9781509041176},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
    mendeley-groups = {NTT/ASRU17},
    pages = {4845--4849},
    title = {{Very deep convolutional networks for end-to-end speech recognition}},
    url = {http://arxiv.org/abs/1610.03022},
    year = {2017}
}
@article{Bahdanau2016b,
    abstract = {Often, the performance on a supervised machine learning task is evaluated with a emph{\{}task loss{\}} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a emph{\{}surrogate loss{\}} function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call emph{\{}consistency with the task loss{\}}. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant {\~{}}13{\%} relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling.},
    archivePrefix = {arXiv},
    arxivId = {1511.06456},
    author = {Bahdanau, Dzmitry and Serdyuk, Dmitriy and Brakel, Phil{\'{e}}mon and Ke, Nan Rosemary and Chorowski, Jan and Courville, Aaron and Bengio, Yoshua},
    eprint = {1511.06456},
    file = {:C$\backslash$:/Users/skarita/Downloads/paper.pdf:pdf},
    journal = {International Conference on Learning Representation (ICLR) Workshop},
    mendeley-groups = {NTT/ASRU17},
    month = {nov},
    pages = {1--13},
    title = {{Task loss estimation for sequence prediction}},
    url = {http://arxiv.org/abs/1511.06456},
    year = {2016}
}
@article{Tsochantaridis:2005,
 author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
 title = {Large Margin Methods for Structured and Interdependent Output Variables},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {1453--1484},
 numpages = {32},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1088722},
 acmid = {1088722},
 publisher = {JMLR.org},
} 
@inproceedings{Miao2015,
    abstract = {The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.},
    author = {Miao, Yajie and Gowayyed, Mohammad and Metze, Florian},
    booktitle = {2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
    doi = {10.1109/ASRU.2015.7404790},
    file = {:C$\backslash$:/Users/skarita/Downloads/1507.08240.pdf:pdf;:C$\backslash$:/Users/skarita/Downloads/07404790.pdf:pdf},
    isbn = {978-1-4799-7291-3},
    keywords = {Acoustics,CTC objective function,Computational modeling,Decoding,EESEN framework,Hidden Markov models,Recurrent neural network,Recurrent neural networks,Speech,Training,WER,WFST-based decoding,connectionist temporal classification,deep RNN model,end-to-end ASR,end-to-end speech recognition,pattern classification,recurrent neural nets,recurrent neural networks,speech recognition,weighted finite-state transducer,word error rate},
    mendeley-groups = {NTT/ASRU17},
    month = {dec},
    pages = {167--174},
    publisher = {IEEE},
    title = {{EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding}},
    url = {http://ieeexplore.ieee.org/document/7404790/},
    year = {2015}
}
@inproceedings{Schulman2015,
    author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
    title = {Gradient estimation using stochastic computation graphs},
    booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
    series = {NIPS'15},
    year = {2015},
    location = {Montreal, Canada},
    pages = {3528--3536},
    numpages = {9},
    url = {http://dl.acm.org/citation.cfm?id=2969442.2969633},
    acmid = {2969633},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
} 
@article{Williams1992,
    abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediatereinforcement tasks and certain limited forms of delayedreinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
    author = {Williams, Ronald J.},
    doi = {10.1023/A:1022672621406},
    file = {:home/skarita/Downloads/williams-92.pdf:pdf},
    isbn = {0885-6125},
    issn = {08856125},
    journal = {Machine Learning},
    keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
    mendeley-groups = {NTT/ASRU17},
    number = {3/4},
    pages = {229--256},
    pmid = {903},
    title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
    url = {http://link.springer.com/10.1023/A:1022672621406},
    volume = {8},
    year = {1992}
}
@article{Sutton1999,
    abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
    author = {Sutton, Richard S. and Mcallester, David and Singh, Satinder and Mansour, Yishay},
    doi = {10.1.1.37.9714},
    file = {:home/skarita/Downloads/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf:pdf;:home/skarita/Downloads/PolicyGradientNIPS99.pdf:pdf},
    isbn = {0-262-19450-3},
    issn = {0047-2875},
    journal = {In Advances in Neural Information Processing Systems 12},
    mendeley-groups = {NTT/ASRU17},
    pages = {1057--1063},
    title = {{Policy gradient methods for reinforcement learning with function approximation}},
    year = {1999}
}

